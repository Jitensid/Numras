{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class Numras():\n    \n    def __init__(self, layer_dims, activation_functions, parameter_type = \"default\",optimizer = None):\n        self.layer_dims = layer_dims\n        self.L = len(self.layer_dims)\n        self.activation_functions = activation_functions\n        self.parameter_type = parameter_type\n        self.parameters = dict()\n        self.optimizer = optimizer\n        \n    def init_parameters(self):\n        \n        for i in range(1, self.L):\n            \n            self.parameters[\"b\" + str(i)] = np.zeros((self.layer_dims[i],1))\n            \n            if self.parameter_type == \"he\":\n                self.parameters[\"W\" + str(i)] = np.random.randn(self.layer_dims[i],self.layer_dims[i-1]) * np.sqrt(2 / self.layer_dims[i-1])\n                \n            elif self.parameter_type == \"xavier\":\n                self.parameters[\"W\" + str(i)] = np.random.randn(self.layer_dims[i],self.layer_dims[i-1]) * np.sqrt(2 / (self.layer_dims[i] + self.layer_dims[i-1]))\n                \n            elif self.parameter_type == \"default\":\n                self.parameters[\"W\" + str(i)] = np.random.randn(self.layer_dims[i],self.layer_dims[i-1]) * 0.01\n               \n            \n    def sigmoid_function(self, Z):\n        res = 1 /(1 + (1 / np.exp(Z)))      \n        return res\n            \n    def relu_function(self,Z):\n        return np.maximum(0,Z)\n    \n    def softmax_function(self,Z):\n        value = np.exp(Z - np.max(Z))\n        res = value / np.sum(value,axis = 0)\n        return res\n    \n    def sigmoid_derivative(self, Z):\n        x = self.sigmoid_function(Z)\n        return x * (1 - x)\n    \n    def relu_derivative(self, Z):\n        Z[Z <= 0] = 0\n        Z[Z > 0] = 1\n        return Z\n    \n    def forward_prop(self,X):\n        \n        cache = dict()\n        \n        cache[\"A\" + str(0)] = X\n        \n        A = X\n        \n        for i in range(1, len(self.layer_dims)):\n            Weight = self.parameters[\"W\" + str(i)]\n            bias = self.parameters[\"b\" + str(i)]\n            \n            Z = np.dot(Weight, A) + bias\n            \n            if self.activation_functions[i-1] == \"relu\":\n                A = self.relu_function(Z)\n            \n            elif self.activation_functions[i-1] == \"softmax\":\n                A = self.softmax_function(Z)\n            \n            elif self.activation_functions[i-1] == \"sigmoid\":\n                A = self.sigmoid_function(Z)\n            \n            cache[\"A\" + str(i)] = A\n            cache[\"Z\" + str(i)] = Z\n            \n        AL = cache[\"A\" + str(len(self.layer_dims) - 1)]\n        \n        return AL, cache\n    \n    def backward_prop(self, AL,cache, X, Y):\n        \n        grads = dict()\n        \n        dZ = None\n        \n        for i in reversed(range(1, len(self.layer_dims))):\n            \n            if self.activation_functions[i-1] == \"relu\":\n                dA = np.dot(self.parameters[\"W\" + str(i+1)].T, dZ)\n                dZ = dA * self.relu_derivative(cache[\"Z\" + str(i)])\n\n            elif self.activation_functions[i-1] == \"sigmoid\":\n                dA = np.dot(self.parameters[\"W\" + str(i+1)].T, dZ)\n                dZ = dA * self.sigmoid_derivative(cache[\"Z\" + str(i)])\n            \n            elif self.activation_functions[i-1] == \"softmax\":\n                dZ = AL - Y\n                \n            grads[\"dW\" + str(i)] = np.dot(dZ, cache[\"A\" + str(i-1)].T) / dZ.shape[1]\n            grads[\"db\" + str(i)] = np.sum(dZ,axis = 1, keepdims = True) / dZ.shape[1]\n     \n        return grads\n    \n    def find_cost(self,AL,Y):\n\n        cost = - np.mean(Y * np.log(AL + 1e-8))\n        \n        return np.squeeze(cost)\n    \n    def predict(self,X,Y):\n        \n        AL, cache = self.forward_prop(X)\n        \n        A = np.argmax(AL, axis = 0)\n        \n        Y_pred = np.argmax(Y, axis = 0)\n        \n        return accuracy_score(A, Y_pred)*100\n    \n    def update_params(self,grads, learning_rate):\n        \n        for i in range(1,len(self.layer_dims)):\n            self.parameters[\"W\" + str(i)] = self.parameters[\"W\" + str(i)] - learning_rate * grads[\"dW\" + str(i)]\n            self.parameters[\"b\" + str(i)] = self.parameters[\"b\" + str(i)] - learning_rate * grads[\"db\" + str(i)]\n    \n    def random_mini_batches(self, X, Y, mini_batch_size):\n        m = X.shape[1]\n        mini_batches = []\n        \n        # Step 1: Shuffle (X, Y)\n        permutation = list(np.random.permutation(m))\n        shuffled_X = X[:, permutation]\n        shuffled_Y = Y[:, permutation]\n\n        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n        num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n        for k in range(0, num_complete_minibatches):\n            mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n            mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n            mini_batch = (mini_batch_X, mini_batch_Y)\n            mini_batches.append(mini_batch)\n\n        # Handling the end case (last mini-batch < mini_batch_size)\n        if m % mini_batch_size != 0:\n            k = m // mini_batch_size\n            mini_batch_X = shuffled_X[:, k*mini_batch_size :]\n            mini_batch_Y = shuffled_Y[:, k*mini_batch_size :]\n            mini_batch = (mini_batch_X, mini_batch_Y)\n            mini_batches.append(mini_batch)\n\n        return mini_batches\n    \n    def plot_cost(self,costs, epochs):\n        ax = sns.lineplot(x =range(1,epochs + 1), y = costs)\n        ax.set(xlabel='Epochs', ylabel='Cost')\n        plt.show()\n    \n    def fit(self,X_train,Y_train,X_test,Y_test,learning_rate,epochs,batch_size):\n        \n        t = 0\n        \n        self.init_parameters()\n        \n        if self.optimizer:\n            self.optimizer.init_params(self.layer_dims)\n\n        costs = list()\n            \n        for i in range(1, epochs + 1):\n            \n            np.random.seed(i)\n            mini_batches = self.random_mini_batches(X_train, Y_train, batch_size)\n            \n            cost = 0\n            \n            for minibatch in mini_batches:\n\n                minibatch_X, mini_batch_Y = minibatch\n\n                AL, cache = self.forward_prop(minibatch_X)\n\n                cost += self.find_cost(AL, mini_batch_Y)\n                                \n                grads = self.backward_prop(AL, cache, minibatch_X, mini_batch_Y)\n\n                if not self.optimizer:\n                    self.update_params(grads,learning_rate)\n\n                else:\n                    \n                    if self.optimizer.optimizer_name() == \"Adam\":\n                        t = t + 1\n                        self.parameters = self.optimizer.update_params_using_Adam(grads, t, self.layer_dims, self.parameters)\n\n                    elif self.optimizer.optimizer_name() == \"RMS\":\n                        self.parameters = self.optimizer.update_params_using_RmsProp(grads, self.layer_dims, self.parameters)            \n                     \n                    elif self.optimizer.optimizer_name() == \"Adamax\":\n                        t = t + 1\n                        self.parameters = self.optimizer.update_params_using_Adamax(grads, t, self.layer_dims, self.parameters)\n                       \n                    \n            costs.append(cost/len(mini_batches))\n            \n            print(\"Epoch \" + str(i) + \" Train Accuracy == \",self.predict(X,Y), \"Test Accuracy == \",self.predict(X_test,Y_test))\n\n        self.plot_cost(costs,epochs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras.datasets import mnist, fashion_mnist\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nimport sklearn.datasets\nimport math\nfrom numba import jit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Adam():\n    \n    def __init__(self, learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = 1e-7\n        self.v = dict()\n        self.s = dict()\n     \n    def optimizer_name(self):\n        return \"Adam\"\n    \n    def init_params(self,layer_dims):\n        \n        for i in range(1,len(layer_dims)):\n            \n            self.v[\"dW\" + str(i)] = np.zeros((layer_dims[i], layer_dims[i-1]))\n            self.v[\"db\" + str(i)] = np.zeros((layer_dims[i], 1))\n            \n            self.s[\"dW\" + str(i)] = np.zeros((layer_dims[i], layer_dims[i-1]))\n            self.s[\"db\" + str(i)] = np.zeros((layer_dims[i], 1))\n            \n    def update_params_using_Adam(self, grads, t, layer_dims,parameters):\n        \n        new_learning_rate = self.learning_rate * np.divide(np.sqrt(1 - np.power(self.beta2, t)), 1- np.power(self.beta1, t))\n        \n        new_parameters = dict()\n        \n        for i in range(1, len(layer_dims)):\n        \n            self.v[\"dW\" + str(i)] = self.beta1 * self.v[\"dW\" + str(i)] + (1 - self.beta1) * grads[\"dW\" + str(i)]\n\n            self.v[\"db\" + str(i)] = self.beta1 * self.v[\"db\" + str(i)] + (1 - self.beta1) * grads[\"db\" + str(i)]\n\n            self.s[\"dW\" + str(i)] = self.beta2 * self.s[\"dW\" + str(i)] + (1 - self.beta2) * grads[\"dW\" + str(i)] ** 2\n\n            self.s[\"db\" + str(i)] = self.beta2 * self.s[\"db\" + str(i)] + (1 - self.beta2) * grads[\"db\" + str(i)] ** 2\n\n            new_parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - new_learning_rate * np.divide(self.v[\"dW\" + str(i)], np.sqrt(self.s[\"dW\" + str(i)]) + self.epsilon) \n            \n            new_parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - new_learning_rate * np.divide(self.v[\"db\" + str(i)], np.sqrt(self.s[\"db\" + str(i)]) + self.epsilon)\n            \n        return new_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Adamax():\n    \n    def __init__(self, learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = 1e-7\n        self.v = dict()\n        self.s = dict()\n     \n    def optimizer_name(self):\n        return \"Adamax\"\n    \n    def init_params(self,layer_dims):\n        \n        for i in range(1,len(layer_dims)):\n            \n            self.v[\"dW\" + str(i)] = np.zeros((layer_dims[i], layer_dims[i-1]))\n            self.v[\"db\" + str(i)] = np.zeros((layer_dims[i], 1))\n            \n            self.s[\"dW\" + str(i)] = np.zeros((layer_dims[i], layer_dims[i-1]))\n            self.s[\"db\" + str(i)] = np.zeros((layer_dims[i], 1))\n            \n    def update_params_using_Adamax(self, grads, t, layer_dims,parameters):\n        \n        new_learning_rate = self.learning_rate / (1 - self.beta1 ** t)\n        \n        new_parameters = dict()\n                \n        for i in range(1, len(layer_dims)):\n        \n            self.v[\"dW\" + str(i)] = self.beta1 * self.v[\"dW\" + str(i)] + (1 - self.beta1) * grads[\"dW\" + str(i)]\n\n            self.v[\"db\" + str(i)] = self.beta1 * self.v[\"db\" + str(i)] + (1 - self.beta1) * grads[\"db\" + str(i)]\n\n            self.s[\"dW\" + str(i)] = np.maximum(self.beta2 * self.s[\"dW\" + str(i)], np.absolute(grads[\"dW\" + str(i)]))\n\n            self.s[\"db\" + str(i)] = np.maximum(self.beta2 * self.s[\"db\" + str(i)], np.absolute(grads[\"db\" + str(i)]))\n           \n            new_parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - new_learning_rate * np.divide(self.v[\"dW\" + str(i)], self.s[\"dW\" + str(i)] + self.epsilon) \n            \n            new_parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - new_learning_rate * np.divide(self.v[\"db\" + str(i)], self.s[\"db\" + str(i)] + self.epsilon)\n            \n        return new_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RmsProp():\n    def __init__(self,learning_rate = 0.1,beta = 0.9):\n        self.learning_rate = learning_rate\n        self.beta = beta\n        self.epsilon = 1e-8\n        self.v = dict()\n        \n    def optimizer_name(self):\n        return \"RMS\"\n        \n    def init_params(self,layer_dims):\n\n        for i in range(1,len(layer_dims)):\n\n            self.v[\"dW\" + str(i)] = np.zeros((layer_dims[i], layer_dims[i-1]))\n            self.v[\"db\" + str(i)] = np.zeros((layer_dims[i], 1))\n\n    def update_params_using_RmsProp(self,grads,layer_dims,parameters):\n        new_parameters = dict()\n\n        for i in range(1,len(layer_dims)):\n            \n            self.v[\"dW\" + str(i)] = self.beta * self.v[\"dW\" + str(i)] + (1 - self.beta) * grads[\"dW\" + str(i)]\n\n            self.v[\"db\" + str(i)] = self.beta * self.v[\"db\" + str(i)] + (1 - self.beta) * grads[\"db\" + str(i)]\n\n            new_parameters[\"W\" + str(i)] = parameters[\"W\" + str(i)] - self.learning_rate * self.v[\"dW\" + str(i)]\n\n            new_parameters[\"b\" + str(i)] = parameters[\"b\" + str(i)] - self.learning_rate * self.v[\"db\" + str(i)]\n            \n        return new_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\nenc = OneHotEncoder(sparse=False, categories='auto')\n\nX = trainX.reshape(trainX.shape[0], trainX.shape[1]*trainX.shape[2]).T\nY = trainY.reshape(-1, trainY.shape[0])\nY = enc.fit_transform(Y.reshape(-1,1)).T\ntest_x = testX.reshape(testX.shape[0], testX.shape[1]*testX.shape[2]).T\ntest_y = testY.reshape(-1, testY.shape[0])\ntest_y = enc.transform(test_y.reshape(-1, 1)).T\n\nX = X / 255\ntest_x = test_x / 255\n\nprint(f\"X shape - {X.shape}\")\nprint(f\"Y shape - {Y.shape}\")\nprint(f\"test_x shape - {test_x.shape}\")\nprint(f\"test_y shape - {test_y.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_dims = [784,64,10]\n\nactivations = [\"relu\",\"softmax\"]\n\nopt = Adamax()\n\n# opt = Adam()\n\n# opt = RmsProp()\n\n# opt = None\n\nmodel = Numras(layer_dims,activations,parameter_type=\"he\",optimizer=opt)\n\nmodel.fit(X,Y,test_x,test_y,0.01,10,batch_size = 1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Flatten\n\nfrom keras.models import Sequential\n\nfrom keras.initializers import glorot_normal, glorot_uniform, he_uniform, he_normal\n\nfrom keras.optimizers import Adam as keras_adam\n\nfrom keras.optimizers import Adamax as keras_adamax\n\nfrom keras.optimizers import RMSprop as keras_Rms_prop\n\nfrom keras.optimizers import Adagrad as keras_adagrad\n\nfrom keras.optimizers import SGD as keras_sgd\n\nmodel = Sequential()\nmodel.add(Dense(64, activation=\"relu\"))\nmodel.add(Dense(10, activation = \"softmax\"))\n\n\nopt1 = keras_adamax(learning_rate=0.001)\n\n# opt1 = keras_Rms_prop(learning_rate=0.001)\n\nmodel.compile(optimizer=opt1,loss=\"categorical_crossentropy\",metrics=[\"accuracy\"],initializers=he_normal())\n\nhistory = model.fit(X.T,Y.T,validation_data=(test_x.T,test_y.T),epochs=10,batch_size = 1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = history.history[\"loss\"]\n\nx = [i for i in range(1,11)]\n\nfig, axs = plt.subplots(ncols=2)\n\nax = sns.lineplot(x = x, y = y,ax=axs[0])\n\nax.set(xlabel='Epochs', ylabel='Cost')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}